{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe6c29e-55fe-4114-95de-5efa46cbb90c",
   "metadata": {},
   "source": [
    "# CLIP-as-service: Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49730b68-de7f-44d3-924f-f38adef6786d",
   "metadata": {},
   "source": [
    "### Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367774ed-0f0a-41e0-97eb-0b5b49bb5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q clip-client \n",
    "!pip install -q ipywidgets # look nice in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b9d77-0292-414a-ab69-7c831323dc73",
   "metadata": {},
   "source": [
    "## Text-to-image cross-modal search\n",
    "\n",
    "Let's build a text-to-image search using CLIP-as-service. Namely, user input a sentence and the program returns the matched images. We will use Totally Looks Like dataset and DocArray package. Note that DocArray is included within clip-client as an upstream dependency, so you don't need to install it separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d45b23-f970-4494-9c50-d06b1ba9bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG: Disable warnings otherwise screen gets flashy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# BUG: Install matplotlib for sprite render\n",
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fb7af0-ff3f-4c56-aa62-ecb65f32f048",
   "metadata": {},
   "source": [
    "### Pull pre-computed embeddings\n",
    "\n",
    "Since you may be running this notebook on a laptop and not a GPU-powered beast, we'll skip the [dataset encoding](https://github.com/jina-ai/clip-as-service/#encode-images) and just download pre-computed embeddings from Jina Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca7cd7-2d04-4799-9185-4c1ed851a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import DocumentArray\n",
    "\n",
    "img_da = DocumentArray.pull('ttl-embedding', show_progress=True, local_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500fda0-3ee6-4d1c-8a1e-5d18b6a71860",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_da.plot_image_sprites()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be3478-bc75-4be6-9fee-2709e2ce6e10",
   "metadata": {},
   "source": [
    "### Connect to CLIP server\n",
    "\n",
    "Be sure to run [`server.ipynb`](./server.ipynb) and take note of the server settings there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e87bf-2a16-4098-96db-10a5273abb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clip_client import Client\n",
    "\n",
    "host = \"grpc://examples.jina.ai:51000\"\n",
    "\n",
    "c = Client(host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456b432a-7ed8-47d7-ad23-42683ad3715a",
   "metadata": {},
   "source": [
    "### Find matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f97805-5867-4bc1-9670-be05495b56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"a happy potato\",\n",
    "    \"professor cat is very serious\",\n",
    "    \"there will be no tomorrow so lets eat unhealthy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e9cb59-253e-4b53-a176-07e1bc88bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt in input_texts:\n",
    "    print(txt)\n",
    "    vec = c.encode([txt])\n",
    "    r = img_da.find(query=vec, limit=9)\n",
    "    r.plot_image_sprites()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c130474-220b-4e17-adee-7c2a892663a0",
   "metadata": {},
   "source": [
    "## Image-to-text cross-modal search\n",
    "\n",
    "We can also switch the input and output of the last program to achieve image-to-text search. Precisely, given a query image find the sentence that best describes the image.\n",
    "\n",
    "We'll sample 10 images from our image DocumentArray and return the closest matching sentences from *Pride and Prejudice*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8f238-2967-4484-9b59-d58360393346",
   "metadata": {},
   "source": [
    "### Download *Pride and Prejudice*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75251bad-d64b-43c7-9162-17720def96e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_da = DocumentArray.pull('ttl-textual', show_progress=True, local_cache=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315136db-1ac8-4a7d-b612-756635a10eec",
   "metadata": {},
   "source": [
    "### Plot matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e36b4-d169-4cd2-aaa5-7a4deba61e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in img_da.sample(10):\n",
    "    d.plot()\n",
    "    results = txt_da.find(d.embedding, limit=1)\n",
    "    \n",
    "    for match in results:\n",
    "        print(match.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c49479-bda6-4e6c-9974-c0c89e1b70d3",
   "metadata": {},
   "source": [
    "### This...isn't great?\n",
    "\n",
    "- We broke down *Pride and Prejudice* into sentences. Our parser recognized things like `Mr.` as one sentence as it has a `.` at the end. So `Mr` is seen as a valid search term and it's so vague it just presents random dudes.\n",
    "- Likewise, a lot of sentences have people's names. There are so many \"Janes\" that \"Jane\" could look like anyone!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
