{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d4ff3b-022c-492a-a0d5-65f11d05dcb4",
   "metadata": {},
   "source": [
    "# üìâ Finetuning Our Model for Better Results\n",
    "\n",
    "In our previous notebook we [built a simple fashion search engine using Docarray](https://colab.research.google.com/github/alexcg1/neural-search-notebooks/blob/main/fashion-search/1_build_basic_search/basic_search.ipynb).\n",
    "\n",
    "Now we'll finetune our model using [Jina Finetuner](https://finetuner.jina.ai) to deliver better results!\n",
    "\n",
    "### The skinny on Jina Finetuner\n",
    "\n",
    "Finetuner lets you tune the weights of any deep neural network for better embeddings on search tasks. It accompanies [Jina](https://github.com/jina-ai/jina) to deliver the last mile of performance for domain-specific neural search applications.\n",
    "\n",
    "üéõ **Designed for finetuning**: a human-in-the-loop deep learning tool for leveling up your pretrained models in domain-specific neural search applications.\n",
    "\n",
    "üî± **Powerful yet intuitive**: all you need is finetuner.fit() - a one-liner that unlocks rich features such as siamese/triplet network, metric learning, self-supervised pretraining, layer pruning, weights freezing, dimensionality reduction.\n",
    "\n",
    "‚öõÔ∏è **Framework-agnostic**: promise an identical API & user experience on PyTorch, Tensorflow/Keras and PaddlePaddle deep learning backends.\n",
    "\n",
    "üßà **[DocArray](https://docarray.jina.ai) integration**: buttery smooth integration with DocArray, reducing the cost of context-switch between experiment and production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdaade9-7845-47b9-8c3d-34ecbdc78467",
   "metadata": {},
   "source": [
    "##  1Ô∏è‚É£ Before you start\n",
    "\n",
    "If you're in Colab, ensure you have GPU selected as runtime. This will speed up processing. You can find it in *Runtime* ‚ñ∂Ô∏è *Change runtime type*\n",
    "\n",
    "![](https://github.com/alexcg1/neural-search-notebooks/raw/main/fashion-search/2_finetune_model/images/runtime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505310b-afca-42fd-98ae-a6298d915498",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "581660fa-3372-4b59-802a-80cb906285b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "except:\n",
    "    in_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca05e5-9609-4c15-89bc-ef3616380a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision~=0.11\n",
    "!pip install finetuner==0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a76be3-4607-4a2b-815b-75b2749e6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import Document, DocumentArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e62d3-7001-493b-a718-bd3de8a9ea13",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Load images\n",
    "\n",
    "This is just the same process we followed in the last notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c4c90-3fd6-406b-a0d5-fce2b09b720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "DATA_PATH = f\"{DATA_DIR}/*.jpg\"\n",
    "MAX_DOCS = 1000\n",
    "\n",
    "# Toy data - If data dir doesn't exist, we'll get data of ~800 fashion images from here\n",
    "TOY_DATA_URL = \"https://github.com/alexcg1/neural-search-notebooks/blob/main/fashion-search/data.zip?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c94fc-9af1-40b2-be7a-17091c3077c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images if they don't exist\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(DATA_DIR) and not os.path.islink(DATA_DIR):\n",
    "    print(f\"Can't find {DATA_DIR}. Downloading toy dataset\")\n",
    "    !wget \"$TOY_DATA_URL\" -O data.zip\n",
    "    !unzip -q data.zip # Don't print out every darn filename\n",
    "    !rm -f data.zip\n",
    "else:\n",
    "    print(f\"Nothing to download. Using {DATA_DIR} for data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281eb32b-a164-4c01-874d-845a09b7359b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = DocumentArray.from_files(DATA_PATH, size=MAX_DOCS)\n",
    "print(f\"{len(docs)} Documents in DocumentArray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2b05c-9173-4326-8bc8-306363223053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(doc):\n",
    "    return (\n",
    "        doc.load_uri_to_image_tensor(80, 60) # input images are 60x80 px\n",
    "        .set_image_tensor_normalization()\n",
    "        .set_image_tensor_channel_axis(-1, 0)\n",
    "    )\n",
    "\n",
    "\n",
    "docs.apply(preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2c73f-850a-4a27-b1c5-dae65a5e14fe",
   "metadata": {},
   "source": [
    "## üß† Load model\n",
    "\n",
    "Again, we're playing the same old song, loading a model just like we did last time. Once again we're using trusty old `resnet50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4d290-12ce-4c49-80c1-d11f3fbb305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f522e8-c0cf-4e5c-9102-7d687052b1d0",
   "metadata": {},
   "source": [
    "### See embeddings\n",
    "\n",
    "Let's take a look at our embeddings to see how good the current (un-finetuned) model is. In Google Colab we need to install some extra libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ec1fe-77a0-4550-8a1d-a8dba13b5f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "docs.embed(model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927a7ab-fbdc-4dab-b5bb-ccf74c40a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not in_colab: # Colab needs way too many dependencies installed for this\n",
    "    docs.plot_embeddings(image_sprites=True, image_source=\"uri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f64747-90f4-469a-a28a-1d2d7355c02e",
   "metadata": {},
   "source": [
    "As we can see, most items are in more or less the position you'd expect, and are clustered according to type.\n",
    "\n",
    "‚ö†Ô∏è To continue, stop the notebook (since the embedding animation blocks the script), then continue from this cell with:\n",
    "\n",
    "- *Runtime* ‚ñ∂Ô∏è *Run after* (in Google Colab)\n",
    "- *Run* ‚ñ∂Ô∏è *Run selected cell and all below* (In Jupyter Lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63caffa-3aa5-4d50-aac9-3ebedd4cb16f",
   "metadata": {},
   "source": [
    "### Examine layers\n",
    "\n",
    "Resnet is a classification model. However, we don't want to train the final (classification) layer, but rather the embedding before that. As we can see below, that layer is called `adaptiveavgpool2d_173` for `resnet50`. We'll set this as our `LAYER_NAME` variable which we'll later use in `ft.fit()`.\n",
    "\n",
    "---\n",
    "\n",
    "‚ÑπÔ∏è Different models will have different layer names. So if you used `resnet18` (for example), your `LAYER_NAME` would be `adaptiveavgpool2d_67`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33513060-a207-4999-99b6-c6a7c40f8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the layers\n",
    "import finetuner as ft\n",
    "ft.display(model, (3, 80, 60))\n",
    "\n",
    "LAYER_NAME = \"adaptiveavgpool2d_173\" # second to last layer name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a11896-e618-4a57-9f82-b48c4c433964",
   "metadata": {},
   "source": [
    "## üìâ Finetune model\n",
    "\n",
    "Here's where the new stuff kicks in!\n",
    "\n",
    "We'll:\n",
    "\n",
    "- Set some basic parameters\n",
    "- Install a module to see progress\n",
    "- Finetune our model, focusing on the embedding layer *just* before the classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c78255-a603-44e5-a372-9eb1226ee683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "EPOCHS = 6         # higher = more time, better finetuning\n",
    "BATCH_SIZE = 64    # higher = use more memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9fb1cf-f9cb-4233-a00d-5ecea5144a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See progress bar in notebook\n",
    "!pip install -q ipywidgets # -q = quiet\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46928d17-b7bd-4795-9696-7ce3525593de",
   "metadata": {},
   "source": [
    "While tuning, keep an eye on the loss rate. It starts at about 0.36 then falls to about 0.08 after 6 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ac4a2-5ee9-4693-86b5-627340aa2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = ft.fit(\n",
    "    model=model,\n",
    "    train_data=docs,\n",
    "    loss='TripletLoss',\n",
    "    epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    to_embedding_model=True,\n",
    "    input_size=(3, 80, 60),\n",
    "    layer_name=LAYER_NAME, # layer before fc as feature extractor\n",
    "    freeze=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba3e958-f738-4a44-9bf4-542da0304c02",
   "metadata": {},
   "source": [
    "### See embeddings\n",
    "\n",
    "Now that we've tuned the model, let's clear out our old embeddings and look at the new ones. \n",
    "\n",
    "‚ö†Ô∏è Unfortunately this doesn't play nice in Google Colab, only Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93591c4a-2d82-44d7-ae1f-8ee9a1630f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'in_colab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1676205/3833869494.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_colab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'in_colab' is not defined"
     ]
    }
   ],
   "source": [
    "if not in_colab:\n",
    "    for doc in docs:\n",
    "        doc.embedding = None\n",
    "    \n",
    "    docs.embed(model, device=DEVICE)\n",
    "    docs.plot_embeddings(image_sprites=True, image_source=\"uri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be37baa1-ff31-4ab3-b0e6-a0db3f40d6d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now you can see a much starker delineation between different clothing types, showing the model has been tuned effectively.\n",
    "\n",
    "‚ö†Ô∏è If you can see the embedding animation above, you'll need to stop the notebook (since the embedding animation blocks the script), then continue from this cell with:\n",
    "\n",
    "- *Runtime* ‚ñ∂Ô∏è *Run after* (in Google Colab)\n",
    "- *Run* ‚ñ∂Ô∏è *Run selected cell and all below* (In Jupyter Lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb65c8-e54d-46e3-aca2-6921eb5064fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üíæ Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978b4c8-d33b-4ef3-b178-3cd787688c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tuned_model, \"tuned-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9629e25-74ae-4e0d-b95b-52bb27b12252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab, download to local filesystem\n",
    "if in_colab:\n",
    "    files.download(\"tuned-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f052a98-16df-43ea-8fb8-99da9fcc8c50",
   "metadata": {},
   "source": [
    "## ‚è≠Ô∏è Next steps\n",
    "\n",
    "Next we'll:\n",
    "\n",
    "- Load the tuned model into our original script\n",
    "- Compare results with the base model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
