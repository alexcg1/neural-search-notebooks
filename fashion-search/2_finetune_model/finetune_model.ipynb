{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d4ff3b-022c-492a-a0d5-65f11d05dcb4",
   "metadata": {},
   "source": [
    "# Finetuning our model\n",
    "\n",
    "In the previous notebook we [built a simple fashion search engine using Docarray](https://colab.research.google.com/github/alexcg1/neural-search-notebooks/blob/main/fashion-search/1_build_basic_search/basic_search.ipynb)\n",
    "\n",
    "Now we'll finetune our model to deliver better results!\n",
    "\n",
    "## Important note\n",
    "\n",
    "This code won't run well in a notebook, since we need to run Finetuner on our local machine. Please:\n",
    "\n",
    "- Download this notebook as a Python file to your local machine (*File* > *Download* > *Download .py*)\n",
    "- Install finetuner in a virtual environment (`pip install finetuner`)\n",
    "- Run this script from that directory\n",
    "\n",
    "If you don't follow the above instructions the script will fail since it can't run the Finetuner GUI from within a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505310b-afca-42fd-98ae-a6298d915498",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca05e5-9609-4c15-89bc-ef3616380a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision~=0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a46210-cfce-4345-bd8d-f29b25544080",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/jina-ai/finetuner # Change to stable release later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a76be3-4607-4a2b-815b-75b2749e6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import Document, DocumentArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e62d3-7001-493b-a718-bd3de8a9ea13",
   "metadata": {},
   "source": [
    "## Load images\n",
    "\n",
    "This is just the same process we followed in the last notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c4c90-3fd6-406b-a0d5-fce2b09b720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "DATA_PATH = f\"{DATA_DIR}/images/*.jpg\"\n",
    "MAX_DOCS = 1000\n",
    "\n",
    "# Toy data - If data dir doesn't exist, we'll get data of ~800 fashion images from here\n",
    "TOY_DATA_URL = \"https://github.com/alexcg1/neural-search-notebooks/blob/main/docarray/fashion-search/data.zip?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c94fc-9af1-40b2-be7a-17091c3077c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images if they don't exist\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(DATA_DIR) and not os.path.islink(DATA_DIR):\n",
    "    print(f\"Can't find {DATA_DIR}. Downloading toy dataset\")\n",
    "    !wget \"$TOY_DATA_URL\" -O data.zip\n",
    "    !unzip -q data.zip # Don't print out every darn filename\n",
    "    !rm -f data.zip\n",
    "else:\n",
    "    print(f\"Nothing to download. Using {DATA_DIR} for data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281eb32b-a164-4c01-874d-845a09b7359b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = DocumentArray.from_files(DATA_PATH, size=MAX_DOCS)\n",
    "print(f\"{len(docs)} Documents in DocumentArray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2b05c-9173-4326-8bc8-306363223053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(doc):\n",
    "    return (\n",
    "        doc.load_uri_to_image_tensor(80, 60)\n",
    "        .set_image_tensor_normalization()\n",
    "        .set_image_tensor_channel_axis(-1, 0)\n",
    "    )  # No need for changing channel axes line if you are using tf/keras\n",
    "\n",
    "\n",
    "docs.apply(preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2c73f-850a-4a27-b1c5-dae65a5e14fe",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Again, we're playing the same old song, loading a model just like we did last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4d290-12ce-4c49-80c1-d11f3fbb305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# model = torchvision.models.resnet50(pretrained=True)\n",
    "model = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a11896-e618-4a57-9f82-b48c4c433964",
   "metadata": {},
   "source": [
    "## Finetune model\n",
    "\n",
    "Here's where the new stuff kicks in!\n",
    "\n",
    "We'll:\n",
    "\n",
    "- Set some basic parameters\n",
    "- Install a module to see progress\n",
    "- Finetune our model, focusing on the embedding layer *just* before the classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c78255-a603-44e5-a372-9eb1226ee683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic setup\n",
    "EPOCHS = 1         # higher = more time, better finetuning\n",
    "BATCH_SIZE = 10    # higher = use more memory\n",
    "DEVICE = \"cpu\"     # if gpu, use \"cuda\", else \"cpu\"\n",
    "LAYER_NAME = \"adaptiveavgpool2d_67\" # This will vary based on the model you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9fb1cf-f9cb-4233-a00d-5ecea5144a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See progress bar in notebook\n",
    "!pip install ipywidgets\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ac4a2-5ee9-4693-86b5-627340aa2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import finetuner as ft\n",
    "\n",
    "tuned_model = ft.fit(\n",
    "    model=model,\n",
    "    train_data=docs,\n",
    "    loss='TripletLoss',\n",
    "    epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    to_embedding_model=True,\n",
    "    input_size=(3, 80, 60),\n",
    "    layer_name=LAYER_NAME, # layer before fc as feature extractor\n",
    "    freeze=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb65c8-e54d-46e3-aca2-6921eb5064fa",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978b4c8-d33b-4ef3-b178-3cd787688c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(model, \"tuned-model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
