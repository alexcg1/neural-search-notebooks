{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d4ff3b-022c-492a-a0d5-65f11d05dcb4",
   "metadata": {},
   "source": [
    "# Finetuning our model\n",
    "\n",
    "In our previous notebook we [built a simple fashion search engine using Docarray](https://colab.research.google.com/github/alexcg1/neural-search-notebooks/blob/main/fashion-search/1_build_basic_search/basic_search.ipynb).\n",
    "\n",
    "Now we'll finetune our model to deliver better results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505310b-afca-42fd-98ae-a6298d915498",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca05e5-9609-4c15-89bc-ef3616380a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision~=0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a46210-cfce-4345-bd8d-f29b25544080",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/jina-ai/finetuner # Change to stable release later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a76be3-4607-4a2b-815b-75b2749e6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import Document, DocumentArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e62d3-7001-493b-a718-bd3de8a9ea13",
   "metadata": {},
   "source": [
    "## Load images\n",
    "\n",
    "This is just the same process we followed in the last notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c4c90-3fd6-406b-a0d5-fce2b09b720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "DATA_PATH = f\"{DATA_DIR}/*.jpg\"\n",
    "MAX_DOCS = 1000\n",
    "\n",
    "# Toy data - If data dir doesn't exist, we'll get data of ~800 fashion images from here\n",
    "TOY_DATA_URL = \"https://github.com/alexcg1/neural-search-notebooks/blob/main/fashion-search/data.zip?raw=true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c94fc-9af1-40b2-be7a-17091c3077c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images if they don't exist\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(DATA_DIR) and not os.path.islink(DATA_DIR):\n",
    "    print(f\"Can't find {DATA_DIR}. Downloading toy dataset\")\n",
    "    !wget \"$TOY_DATA_URL\" -O data.zip\n",
    "    !unzip -q data.zip # Don't print out every darn filename\n",
    "    !rm -f data.zip\n",
    "else:\n",
    "    print(f\"Nothing to download. Using {DATA_DIR} for data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281eb32b-a164-4c01-874d-845a09b7359b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = DocumentArray.from_files(DATA_PATH, size=MAX_DOCS)\n",
    "print(f\"{len(docs)} Documents in DocumentArray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2b05c-9173-4326-8bc8-306363223053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(doc):\n",
    "    return (\n",
    "        doc.load_uri_to_image_tensor(80, 60)\n",
    "        .set_image_tensor_normalization()\n",
    "        .set_image_tensor_channel_axis(-1, 0)\n",
    "    )\n",
    "\n",
    "\n",
    "docs.apply(preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2c73f-850a-4a27-b1c5-dae65a5e14fe",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "Again, we're playing the same old song, loading a model just like we did last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4d290-12ce-4c49-80c1-d11f3fbb305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "# model = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33513060-a207-4999-99b6-c6a7c40f8569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the layers\n",
    "import finetuner as ft\n",
    "ft.display(model, (3, 80, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a11896-e618-4a57-9f82-b48c4c433964",
   "metadata": {},
   "source": [
    "## Finetune model\n",
    "\n",
    "Here's where the new stuff kicks in!\n",
    "\n",
    "We'll:\n",
    "\n",
    "- Set some basic parameters\n",
    "- Install a module to see progress\n",
    "- Finetune our model, focusing on the embedding layer *just* before the classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c78255-a603-44e5-a372-9eb1226ee683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Basic setup\n",
    "EPOCHS = 6         # higher = more time, better finetuning\n",
    "BATCH_SIZE = 64    # higher = use more memory\n",
    "# LAYER_NAME = \"adaptiveavgpool2d_67\" # for resnet18\n",
    "LAYER_NAME = \"adaptiveavgpool2d_173\" # for resnet50\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9fb1cf-f9cb-4233-a00d-5ecea5144a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See progress bar in notebook\n",
    "!pip install -q ipywidgets # -q = quiet\n",
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46928d17-b7bd-4795-9696-7ce3525593de",
   "metadata": {},
   "source": [
    "While tuning, keep an eye on the loss rate. It starts at about 0.36 then falls to about 0.08 after 6 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5ac4a2-5ee9-4693-86b5-627340aa2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = ft.fit(\n",
    "    model=model,\n",
    "    train_data=docs,\n",
    "    loss='TripletLoss',\n",
    "    epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    to_embedding_model=True,\n",
    "    input_size=(3, 80, 60),\n",
    "    layer_name=LAYER_NAME, # layer before fc as feature extractor\n",
    "    freeze=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb65c8-e54d-46e3-aca2-6921eb5064fa",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978b4c8-d33b-4ef3-b178-3cd787688c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(tuned_model, \"tuned-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9629e25-74ae-4e0d-b95b-52bb27b12252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in Colab, download to local filesystem\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(\"tuned-model\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f052a98-16df-43ea-8fb8-99da9fcc8c50",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Next we'll:\n",
    "\n",
    "- Load the tuned model into our original script\n",
    "- Compare results with the base model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
