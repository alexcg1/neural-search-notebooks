{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d4ff3b-022c-492a-a0d5-65f11d05dcb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading our Finetuned Model\n",
    "\n",
    "In our previous notebooks we:\n",
    "\n",
    "- [Built a simple image search engine for fashion products using Jina's Docarray library](https://colab.research.google.com/github/alexcg1/neural-search-notebooks/blob/main/fashion-search/1_build_basic_search/basic_search.ipynb)\n",
    "- [Finetuned our model using Jina Finetuner](https://colab.research.google.com/github/alexcg1/neural-search-notebooks/blob/main/fashion-search/2_finetune_model/finetune_model.ipynb)\n",
    "\n",
    "Now we'll integrate our fine-tuned model into our original search engine and compare results\n",
    "\n",
    "Next time we'll build our fashion search engine into something production-ready using [Jina's neural search framework](https://github.com/jina-ai/jina)\n",
    "\n",
    "You can download this notebook from [GitHub](https://github.com/alexcg1/neural-search-notebooks). PRs and issues are always welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b030d-d5ea-4f4c-bf2e-fa9c8b607532",
   "metadata": {},
   "source": [
    "## Before starting\n",
    "\n",
    "1. Ensure you've completed the previous two notebooks.\n",
    "2. Copy the `tuned-model` file from the finetuner tutorial and place it in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8b7c1-50b2-46cc-b8ba-770ff6e1f5f3",
   "metadata": {},
   "source": [
    "## üì∫ Watch the video\n",
    "\n",
    "Get a guided tour of the notebook and search results with Jack from Jina AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e428602f-8a09-49ff-bfaa-9def221bb8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"Amo19S1SrhE\", width=800, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b73db6-b90a-4df3-aa5e-a484d733f81b",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b494794-0efe-401d-81fb-7474cacc3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    in_colab = True\n",
    "except:\n",
    "    in_colab = False\n",
    "\n",
    "DATA_DIR = \"./data\"\n",
    "DATA_PATH = f\"{DATA_DIR}/*.jpg\"\n",
    "MAX_DOCS = 1000\n",
    "QUERY_IMAGE = \"./query.jpg\" # image we'll use to search with\n",
    "PLOT_EMBEDDINGS = False # Really useful but have to manually stop it to progress to next cell\n",
    "\n",
    "# Toy data - If data dir doesn't exist, we'll get data of ~800 fashion images from here\n",
    "TOY_DATA_URL = \"https://github.com/alexcg1/neural-search-notebooks/raw/main/fashion-search/data.zip?raw=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505310b-afca-42fd-98ae-a6298d915498",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca05e5-9609-4c15-89bc-ef3616380a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"docarray[full]==0.4.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a76be3-4607-4a2b-815b-75b2749e6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import Document, DocumentArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e62d3-7001-493b-a718-bd3de8a9ea13",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22aeecf-1ade-40cd-b28f-43808ad46ee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download images if they don't exist\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(DATA_DIR) and not os.path.islink(DATA_DIR):\n",
    "    print(f\"Can't find {DATA_DIR}. Downloading toy dataset\")\n",
    "    !wget \"$TOY_DATA_URL\" -O data.zip\n",
    "    !unzip -q data.zip # Don't print out every darn filename\n",
    "    !rm -f data.zip\n",
    "else:\n",
    "    print(f\"Nothing to download. Using {DATA_DIR} for data\")\n",
    "\n",
    "docs = DocumentArray.from_files(DATA_PATH, size=MAX_DOCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3759fa-887b-43f9-be65-3748e4c0dfba",
   "metadata": {},
   "source": [
    "## üè≠ Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05d7fd-cdf0-4e0a-b243-b98daa6a9ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import Document\n",
    "\n",
    "def preproc(d: Document):\n",
    "    return (d.load_uri_to_image_tensor()  # load\n",
    "             .set_image_tensor_shape((80, 60))  # ensure all images right size (dataset image size _should_ be (80, 60))\n",
    "             .set_image_tensor_normalization()  # normalize color \n",
    "             .set_image_tensor_channel_axis(-1, 0))  # switch color axis for the PyTorch model later\n",
    "\n",
    "docs.apply(preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1414b203-1bf6-4057-a264-a8d88759b85f",
   "metadata": {},
   "source": [
    "## üß† Embed images using original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf166e8-a07e-42aa-8bbe-4ca746280af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision~=0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee2cabf-c6d8-45e7-a16c-802005324668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\"\n",
    "\n",
    "import torchvision\n",
    "model = torchvision.models.resnet50(pretrained=True)  # load ResNet50\n",
    "\n",
    "docs.embed(model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de6557-82e9-4309-a4eb-c69c768809d5",
   "metadata": {},
   "source": [
    "## Create query Document\n",
    "\n",
    "Let's just use the first image from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa11cba-3084-4fc5-a0a9-0db49a8f6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download query doc\n",
    "!wget https://github.com/alexcg1/neural-search-notebooks/raw/main/fashion-search/1_build_basic_search/query.jpg -O query.jpg\n",
    "\n",
    "query_docs = DocumentArray([Document(uri=QUERY_IMAGE)]) # Wrap in a DocumentArray\n",
    "query_docs[0].display()\n",
    "\n",
    "query_docs.apply(preproc)\n",
    "\n",
    "query_docs.embed(model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a29921-b988-49f3-9384-21787655119b",
   "metadata": {},
   "source": [
    "## Get matches and see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237887b-646d-4d9a-88b2-ccafbebf0bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_docs.match(docs, limit=9)\n",
    "\n",
    "(DocumentArray(query_docs[0].matches, copy=True)\n",
    "    .apply(lambda d: d.set_image_tensor_channel_axis(0, -1)\n",
    "                      .set_image_tensor_inv_normalization())).plot_image_sprites()\n",
    "\n",
    "for match in query_docs[0].matches:\n",
    "    print(match.scores[\"cosine\"].value) # print score to see how confident the model is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7cc10-ea77-49fe-a179-66b2fff81a93",
   "metadata": {},
   "source": [
    "## üß† Load new model and embed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6e0bf-aa9f-4031-9577-f645516064d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILENAME = \"tuned-model\"\n",
    "model = torch.load(MODEL_FILENAME)\n",
    "\n",
    "docs = DocumentArray.from_files(DATA_PATH, size=MAX_DOCS)\n",
    "docs.apply(preproc)\n",
    "docs.embed(model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2bfb58-7d06-4628-ab5e-6fa0c0d37e73",
   "metadata": {},
   "source": [
    "## Embed query Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd2f08-3d45-4887-991a-284af1f5bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_docs = DocumentArray([Document(uri=QUERY_IMAGE)])\n",
    "query_docs.apply(preproc)\n",
    "query_docs.embed(model, device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1550a7dd-f7a4-4b84-aa4f-c1caa00ad725",
   "metadata": {},
   "source": [
    "## Get matches and see results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d5bec-dc21-4305-956d-defb7c45cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_docs.match(docs, limit=9)\n",
    "\n",
    "(DocumentArray(query_docs[0].matches, copy=True)\n",
    "    .apply(lambda d: d.set_image_tensor_channel_axis(0, -1)\n",
    "                      .set_image_tensor_inv_normalization())).plot_image_sprites()\n",
    "\n",
    "for match in query_docs[0].matches:\n",
    "    print(match.scores[\"cosine\"].value) # print score to see how confident the model is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
