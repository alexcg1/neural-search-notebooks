{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d4ff3b-022c-492a-a0d5-65f11d05dcb4",
   "metadata": {},
   "source": [
    "# Building a fashion search engine with `docarray`\n",
    "\n",
    "[DocArray](https://docarray.jina.ai) is a library for nested, unstructured data in transit, including text, image, audio, video, 3D mesh, etc. It allows deep-learning engineers to efficiently process, embed, search, recommend, store, and transfer the data with a Pythonic API.\n",
    "\n",
    "In this example we'll use DocArray to build a simple search engine for fashion images. You'll be able to upload an image of some clothing and find similar matches in the dataset.\n",
    "\n",
    "In future notebooks we'll look at:\n",
    "\n",
    "- Improving your model's performance using [Jina Finetuner](https://finetuner.jina.ai)\n",
    "- Expanding and serving your search engine using [Jina's neural search framework](https://github.com/jina-ai/jina) and Executors from [Jina Hub](https://hub.jina.ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b73db6-b90a-4df3-aa5e-a484d733f81b",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We'll set up some basic variables. Feel free to adapt these for your own project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c4c90-3fd6-406b-a0d5-fce2b09b720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "DATA_PATH = f\"{DATA_DIR}/images/*.jpg\"\n",
    "MAX_DOCS = 1000\n",
    "QUERY_IMAGE = \"./query.jpg\" # image we'll use to search with\n",
    "PLOT_EMBEDDINGS = False # Really useful but have to manually stop it to progress to next cell\n",
    "\n",
    "# Toy data - If data dir doesn't exist, we'll get data of ~800 fashion images from here\n",
    "TOY_DATA_URL = \"https://github.com/alexcg1/neural-search-notebooks/blob/main/docarray/fashion-search/data.zip?raw=true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9505310b-afca-42fd-98ae-a6298d915498",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca05e5-9609-4c15-89bc-ef3616380a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use \"[full]\" because we want to deal with more complex data like images (as opposed to text)\n",
    "!pip install \"docarray[full]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a76be3-4607-4a2b-815b-75b2749e6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import Document, DocumentArray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e62d3-7001-493b-a718-bd3de8a9ea13",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c94fc-9af1-40b2-be7a-17091c3077c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images if they don't exist\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(DATA_DIR) and not os.path.islink(DATA_DIR):\n",
    "    print(f\"Can't find {DATA_DIR}. Downloading toy dataset\")\n",
    "    !wget \"$TOY_DATA_URL\" -O data.zip\n",
    "    !unzip -q data.zip # Don't print out every darn filename\n",
    "    !rm -f data.zip\n",
    "else:\n",
    "    print(f\"Nothing to download. Using {DATA_DIR} for data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281eb32b-a164-4c01-874d-845a09b7359b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use `.from_files` to quickly load them into a `DocumentArray`\n",
    "docs = DocumentArray.from_files(DATA_PATH, size=MAX_DOCS)\n",
    "print(f\"{len(docs)} Documents in DocumentArray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098e2ef-ad1b-4fec-918a-7b38f6fb202b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.plot_image_sprites() # Preview the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3759fa-887b-43f9-be65-3748e4c0dfba",
   "metadata": {},
   "source": [
    "## Apply preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa60b17-aa2e-4e04-9858-d09c0c9b6795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docarray import Document\n",
    "\n",
    "# Convert to tensor, normalize so they're all similar enough\n",
    "def preproc(d: Document):\n",
    "    return (d.load_uri_to_image_tensor()  # load\n",
    "             .set_image_tensor_shape((80, 60))  # ensure all images right size (dataset image size _should_ be (80, 60))\n",
    "             .set_image_tensor_normalization()  # normalize color \n",
    "             .set_image_tensor_channel_axis(-1, 0))  # switch color axis for the PyTorch model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef697e23-5505-40ef-99ef-6dc7e99aed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply en masse\n",
    "docs.apply(preproc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1414b203-1bf6-4057-a264-a8d88759b85f",
   "metadata": {},
   "source": [
    "## Embed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf166e8-a07e-42aa-8bbe-4ca746280af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e179dd28-bf2c-45ee-99e5-223e0a144a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\" # Change to \"cpu\" if you don't have GPU (it'll still run fine, just a bit slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c928a0a-cd04-4699-8a3b-29e9416f0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "model = torchvision.models.resnet50(pretrained=True)  # load ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889dd6bc-522c-4ac9-8243-581c644f395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.embed(model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146c742f-5ed7-466c-a3db-fb0b6790c383",
   "metadata": {},
   "source": [
    "### Visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9880f-9f34-4b41-9859-6467474ade10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_EMBEDDINGS:\n",
    "    docs.plot_embeddings(image_sprites=True, image_source=\"uri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de6557-82e9-4309-a4eb-c69c768809d5",
   "metadata": {},
   "source": [
    "## Create query Document\n",
    "\n",
    "Let's just use the first image from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a732acb-b3ea-4115-b2f9-3950912d7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download query doc\n",
    "!wget https://github.com/alexcg1/neural-search-notebooks/raw/main/docarray/fashion-search/query.jpg -O query.jpg\n",
    "\n",
    "query_doc = Document(uri=QUERY_IMAGE)\n",
    "query_doc.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779f8a99-599e-4cf7-9196-e808f4645f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throw the one Document into a DocumentArray, since that's what we're matching against\n",
    "query_docs = DocumentArray([query_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ee56f-56b5-4ccf-b54d-829f2325bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply same preprocessing\n",
    "query_docs.apply(preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f11641-19e0-4baf-94b0-015bdfdb33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and create embedding just like we did with the dataset\n",
    "query_docs.embed(model, device=device) # If running on non-gpu machine, change \"cuda\" to \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a29921-b988-49f3-9384-21787655119b",
   "metadata": {},
   "source": [
    "## Get matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f9826-21c5-4cd2-81a1-494c16289dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_docs.match(docs, limit=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb6f736-c1fc-46b8-96d3-fb017f1a2c7f",
   "metadata": {},
   "source": [
    "## See the results\n",
    "\n",
    "As you can see, the model is finding matches based on the input images - including the human wearing the clothes! In reality we want to match the clothes themselves, so later we'll fine-tune our model using Jina AI's [finetuner](https://finetuner.jina.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ff195-300b-42ac-a14b-88bd9034ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "(DocumentArray(query_doc.matches, copy=True)\n",
    "    .apply(lambda d: d.set_image_tensor_channel_axis(0, -1)\n",
    "                      .set_image_tensor_inv_normalization())).plot_image_sprites()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d6d29-ea24-4445-85f9-ddd458fcaa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT_EMBEDDINGS:\n",
    "    query_doc.matches.plot_embeddings(image_sprites=True, image_source=\"uri\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b363ca8-e700-4a77-978d-a9a0ffb6ed97",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "### Finetuning our model\n",
    "\n",
    "![](https://finetuner.jina.ai/_images/labeler-on-fashion-mnist.gif)\n",
    "\n",
    "In our next notebook we'll improve our model's performance with **[Jina Finetuner](https://finetuner.jina.ai)**\n",
    "\n",
    "### Building into a real world application\n",
    "\n",
    "![](https://github.com/alexcg1/jina-multimodal-fashion-search/raw/main/demo.gif)\n",
    "\n",
    "In a future notebook we'll use **[Jina's neural search framework](https://github.com/jina-ai/jina/)** and **[Jina Hub Executors](https://hub.jina.ai)** to build a [real world fashion search engine](http://examples.jina.ai/fashion) with minimal lines of code.\n",
    "\n",
    "- [Finetune](https://finetuner.jina.ai) our model to improve matching\n",
    "- Build into a real-world search engine with [Jina](https://github.com/jina-ai/jina) (example [here](http://examples.jina.ai/fashion))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
